---
layout: post
comments: true
title:  "CARLA 0.9.7 release"
subtitle: "Pedestrian improvements, pedestrian AI, importing & exporting assets, snapshots, docker build and more!"
description: "This release brings the long-awaited traffic manager, it adds new sensors such as IMU and a Radar, new customizable pedestrian navigation, illumination improvements among other features and fixes."
author: "@fMadecu"
date:   2019-12-11 8:00:00 +0002
image: 'img/carla.jpg'
background: '/img/posts/2019-07-12/Bann001.jpg'
---

<!-- 

Poner una imagen:
![pedestrian_ai](/img/posts/2019-07-12/pedestrians.gif){:class="img-fluid"} 

Poner video:
{% include youtube.html id="TOojcifcRBA" %}

_Cursiva_
**Negrita**
`TextoRojo`

Cuadro código Python
```py
carla.WalkerAIController:
  start()
  stop()
  go_to_location(destination)
  set_max_speed(speed)
```
Pie de imagen
> insertar frase para pie de imagen

Enlace:
[LINK](https://carla.readthedocs.io/en/latest/python_api_tutorial/#pedestrians)!

-->


We are excited to announce the new features included in **CARLA 0.9.7**!

{% include youtube.html id="TOojcifcRBA" %}

{% include release_button.html release_tag="0.9.7" %}

This release brings the long-awaited traffic manager, it adds new sensors such as IMU and a Radar, new customizable pedestrian navigation, illumination improvements among other features and fixes.

Let's take a detailed look at what is new!

## A whole new Traffic Manager!

The Autopilot has been recently replaced by the first version of the Traffic Manager !! Which is built on top of the CARLA API in C++ as a separate module to handle a group of vehicles. Its main objective is to improve the way cars roam around the city, whilst giving users the option to **customize traffic** like never before. To achieve this, we have built an architecture that allows us to pass information through several stages and manage shared data structures.


![Traffic_Manager](/img/posts/2019-12-11/trafficmanager_img.gif){:class="img-fluid"} 


The Traffic Manager allows users to interact with
a list of parameters:

* Distance to leading vehicle
* Allow speed above limit
* % of velocity reduction from the speed limit
* % to respect “keep right” rule
* % of running a traffic light
* Ignore pedestrians
* Ignore other vehicles
* Force a lane change
* Turn lane changes on/off.
* % of overtaking, when possible
* % to avoid tailgating, when possible.


## New Sensors!!

### **Radar**

Description
A radar sensor has been added to Carla. It uses raycasting to detect objects in the level and retrieves its position and velocity. Right now, the point distribution is uniform, this can be changed in the server's code.

As you will see in the API docs, you have available two new classes: `RadarMeasurement` and `RadarDetection` to access the new Radar information.

**Code example**

```py
...
# Get radar blueprint

radar_bp = bp_lb.filter('sensor.other.radar')[0]

# Set Radar attributes, by default are:

radar_bp.set_attribute('horizontal_fov', '30') # degrees

radar_bp.set_attribute('vertical_fov', '30') # degrees

radar_bp.set_attribute('points_per_second', '1500')
radar_bp.set_attribute('range', '100') # meters


# Spawn the radar

radar = world.spawn_actor(radar_bp, carla.Transform())

# Create a radar's callback that just prints the data

def radar_callback(weak_radar, sensor_data):
    self = weak_radar()
    if not self:
        return
    for detection in sensor_data:
        print('depth:    ' + str(detection.depth)) # meters

        print('azimuth:  ' + str(detection.azimuth)) # rad

        print('altitude: ' + str(detection.altitude)) # rad

        print('velocity: ' + str(detection.velocity)) # m/s


weak_radar = weakref.ref(radar)
# Bind the callback

radar.listen(lambda sensor_data: radar_callback(weak_radar, sensor_data))

```

**Visualization example**

The red intensity means negative speed, and the blue one means positive speed in sensor coordinates. Depth, azimuth, and elevation are used to calculate the 3d position of each detection.

![radar_RGB](/img/posts/2019-12-11/Radar_RGB.gif){:class="img-fluid"} 
>Radar speed information with the RGB camera

The example shows a vehicle approaching from the right side of an static radar.

![radar](/img/posts/2019-12-11/radar_img.gif){:class="img-fluid"} 



### **Inertial Measurement Unit (IMU)**

Added support for realtime IMU and Radar data visualization in `manual_control.py`.

New IMU sensor that provides:

* Accelerometer (m/s^2)
* Gyroscope (rad/s)
* Compass (rad)

Added new noise atributions:
```py
sensor.other.imu

    Attributes:
        noise_accel_stddev_x (Float)
        noise_accel_stddev_y (Float)
        noise_accel_stddev_z (Float)
        noise_gyro_bias_x (Float)
        noise_gyro_bias_y (Float)
        noise_gyro_bias_z (Float)
        noise_gyro_stddev_x (Float)
        noise_gyro_stddev_y (Float)
        noise_gyro_stddev_z (Float)
        noise_seed (Int)
```

## Modified sensors

### **Extended RGB sensor**

We've added more control over the RGB camera sensor, so you can work with them as a real camera.

All the currently available RGB camera settings can be modified like this:

```py
# Sensor

camera_bp.set_attribute('sensor_tick', str(0.0))

# Image

camera_bp.set_attribute('image_size_x', str(800))
camera_bp.set_attribute('image_size_y', str(600))
camera_bp.set_attribute('gamma', str(2.2))

# Camera (used for manual exposure and depth of field)

camera_bp.set_attribute('fov', str(90.0))
camera_bp.set_attribute('shutter_speed', str(60.0))
camera_bp.set_attribute('iso', str(1200.0))
camera_bp.set_attribute('fstop', str(1.4)) # Aperture is 1.0 / fstop

camera_bp.set_attribute('min_fstop', str(1.2)) # Maximum Aperture

camera_bp.set_attribute('blade_count', str(5))

# Exposure ("manual" by default)

camera_bp.set_attribute('exposure_mode', str('manual'))
camera_bp.set_attribute('exposure_compensation', str(3.0))

# Exposure - histogram specific (used when "exposure_mode" == "histogram")

camera_bp.set_attribute('exposure_min_bright', str(0.1))
camera_bp.set_attribute('exposure_max_bright', str(2.0))
camera_bp.set_attribute('exposure_speed_up', str(3.0))
camera_bp.set_attribute('exposure_speed_down', str(1.0))
camera_bp.set_attribute('calibration_constant', str(16.0))

# Depth of field

camera_bp.set_attribute('focal_distance', str(1000.0))
camera_bp.set_attribute('blur_amount', str(1.0))
camera_bp.set_attribute('blur_radius', str(0.0))

# Motion Blur

camera_bp.set_attribute('motion_blur_intensity', str(0.45))
camera_bp.set_attribute('motion_blur_max_distortion', str(0.35))
camera_bp.set_attribute('motion_blur_min_object_screen_size', str(0.1))

# Tonemapper

camera_bp.set_attribute('slope', str(0.88))
camera_bp.set_attribute('toe', str(0.55))
camera_bp.set_attribute('shoulder', str(0.26))
camera_bp.set_attribute('black_clip', str(0.0))
camera_bp.set_attribute('white_clip', str(0.04))

# Color

camera_bp.set_attribute('temp', str(6500.0))
camera_bp.set_attribute('tint', str(0.0))

# Other

camera_bp.set_attribute('enable_postprocess_effects', str(True))
```
>_in this example they all are set to the default value_

More information about these attributes can be found in the UE4 official docs:

* [Automatic Exposure](https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/AutomaticExposure/index.html)!

* [Cinematic DOF Methods](https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/DepthOfField/CinematicDOFMethods/index.html)!

* [Color Grading](https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/ColorGrading/index.html)!

### **Lens distortion**

Now all the camera-based sensors (_RGB, Depht and Semantic Segmentation_) are provided with and additional lens distortion shader.
You can see the effect of just increasing the `lens_circle_multiplier` value to `1.0` - `2.0` or so.

```py
camera_bp.set_attribute('lens_circle_falloff', str(5.0))
camera_bp.set_attribute('lens_circle_multiplier', str(0.0))
camera_bp.set_attribute('lens_k', str(-1.0))
camera_bp.set_attribute('lens_kcube', str(0.0))
camera_bp.set_attribute('lens_x_size', str(0.08))
camera_bp.set_attribute('lens_y_size', str(0.08))
```
Also, exposed chromatic aberration for `sensor.camera.rgb`:

```py
rgb_camera_bp.set_attribute('chromatic_aberration_intensity', str(0.5)) # 0.0 by default

rgb_camera_bp.set_attribute('chromatic_aberration_offset', str(0.0))
```
Changed `manual_control.py` so you can visualize a lens distortion example pressing `8`.

### **GNSS**

Moved the GNSS sensor from client to server side.

**Advantages**
* Fewer calculations on the client-side.
* Exactly the same output for different clients listening to the same sensor.

**Limitations**
* More calculations on the server-side.
* More networking traffic.

Added new noise atributions:

```py
sensor.other.gnss

    Attributes:
        noise_alt_bias (Float)
        noise_alt_stddev (Float)
        noise_lat_bias (Float)
        noise_lat_stddev (Float)
        noise_lon_bias (Float)
        noise_lon_stddev (Float)
        noise_seed (Int)
```


## API changes

We had to make some changes in the API in order to implement new functions with sensors. Those are the following changes:

* Lidar: `range` is now set in meters, not in centimeters
* Lidar: `horizontal_angle` is now received in radians, not in degrees
* GNSS: `carla.GnssEvent` renamed to `carla.GnssMeasurement`



## New Pedestrians Navigation

With this new feature users can control and define the path that pedestrians will follow automatically, which can be controlled using CARLA API!

Now it is possible for pedestrians to automatically cross the enabled walkways (sidewalks and crosswalks) using the AI system.

![Pedestrian_Crossing](/img/posts/2019-12-11/pedestrianCrossing_img.gif){:class="img-fluid"} 


It is possible to create your own enabled walkways for pedestrians with:
* Carla Exporter
* Binary Generator





Check it out in our [tutorial](https://carla.readthedocs.io/en/latest/how_to_generate_pedestrians_navigation)!

![Pedestrian_Navigation](/img/posts/2019-12-11/Pedestrian_Navigation.jpg){:class="img-fluid"} 
>Semantic segmentation for pedestrian navigation created with our plug-in _Carla Exporter_ and _Binary Generator_




The user can set a % of pedestrian who will cross randomly over the roads. They can also set the pedestrian speed between walking or running.

```py
# set how many pedestrians can cross the road (10%)

world.set_pedestrians_cross_factor(0.1)
```


```py
if walker_bp.has_attribute('speed'):
  if (random.random() > 0.5):
    # walking

    walker_speed = walker_bp.get_attribute('speed').recommended_values[1]
  else:
    # running
    
    walker_speed = walker_bp.get_attribute('speed').recommended_values[2]
```

The pedestrians are now aware if there is a car near by when they are going to cross a crosswalk and act accordingly.

<!-- Insert GIF stop crossing because a car -->

## Improved Pedestrians


### **New pedestrian animations and controller**

We've improved our pedestrian animation controller, in addition to improving their animations we've added a new animation state. Latest changes in animation included animation clips from Mocap sources, we captured this animation in our own facilities and later we cleaned them on Maya.

The basic controller included walk, run and idle state/animations, there's a new state : pivot. It's a state that covers situations in which our pedestrians are stationary but reorienting themselves to face a different direction. 


![Pedestrian_Animations](/img/posts/2019-12-11/Pedestrian_animations.gif){:class="img-fluid"} 

This added to improvements in pedestrian's navigation, will give a more natural actor's behavior to the simulations.


### **New pedestrian skin shader**

We've made a few improvements with pedestrian's materials. We've implemented SSS (SubSurface Scattering) shader on their skin material. Although the effect is so slight, and is truly appreciated when affected by a backlight, it also improves the result on our scenes, independent of the illumination.

Other material which is affected by backlight and translucency is our hair material, with the UE4 hair shader.

## New illumination

With the new SSAO function, we achieved more precise shadows, boosting the Ambient Occlusion effect in real time.

![SSAO](/img/posts/2019-12-11/ssao_img.gif){:class="img-fluid"} 

We added an HDRi image to illuminate indirect lighting in the scene to achive a more realistic look.

![HDRi](/img/posts/2019-12-11/hdri_img.gif){:class="img-fluid"} 


## Matched cameras between unreal editor and client

In this last update, we’ve tweaked the camera’s values, so now, the Carla client looks the same as Unreal Engine editor and spectator windows.

![Matched_cameras](/img/posts/2019-12-11/diference_img.gif){:class="img-fluid"} 


## Doxygen documentation

Added Doxygen documentation online with automatic updates through Jenkins pipeline.

[Doxygen documentation](http://carla.org/Doxygen/html/index.html)!

## Change Log

- Add build variant with AD RSS library integration with RSS sensor and result visualisation
- Added new sensor: Inertial measurement unit (IMU)
- Added new sensor: Radar
- Moved GNSS sensor from client to server side
- Now all the camera-based sensors are provided with an additional parametrized lens distortion shader
- API changes:
  + Lidar: _range_ is now set in meters, not in centimeters
  + Lidar: _horizontal_angle_ is now received in radians, not in degrees
  + GNSS: _carla.GnssEvent_ renamed to _carla.GnssMeasurement_
- API extensions:
  + Added _carla.IMUMeasurement_
  + Added _carla.RadarMeasurement_ and _carla.RadarDetection_
  + GNSS data can now be obtained with noise
  + IMU data can now be obtained with noise
- Updated manual_control.py with a lens disortion effect example
- Fixed pylint for python3 in travis
- Fixed PointCloudIO _cout_ that interfiered with other python modules
- Better steering in manual control
- Added Doxygen documentation online with automatic updates through Jenkins pipeline
- Fixed client_bounding_boxes.py example script
- Exposed RGB camera attribute in the API: exposure, depth of field, tonemapper, color correction, and chromatic aberration
- Fixed materials and semantic segmentation issues regarding importing assets
- Added TrafficManager to replace autopilot in managing the NPC vehicles
- Fixed ObstacleSensor to return HitDistance instead of HitRadius
- New Feature RoadPainter 
	+ Use opendrive information to paint roads 
	+ Add mask to blend different materials between them
	+ Spawn meshes randomly
	+ Spawn decals randomly
	+ Customize texture properties (Brightness, Hue, Saturation, Normal intensity, Roughness Intensity) for each material.
	+ Optimaze Road Shader by adding the Height map on ORM texture alpha channel Cleaning blueprints Instanced LaneMarking shaders with Road shader
- Decals
	+ 11 Small tar snakes
	+ 4  Big TarSnakes
	+ 4  Big Cracks
	+ 3 Manhole
	+ 1 Grate
	+ 3 RoadPatch

- Navigation Pedestrian
  + Change the nomenclature of mesh for each city to be able to export with the plugin "Export Maps"
  + Add invisible planes in crosswalks to let cross the pedestrians

- Assets
  + Add new meshes Trash: Bag Paper, Bag Plastic, Cigarrete, Can, Cup Paper, Newspaper and leafs
  + New streets lights and traffic signs
  + New vegetation with more detail: Tree and headge

- Pedestrians
  + New Male model
  + Improve animations for each character: idle - walk - run.
  + Tweak animation controller
  + Add Subsurface Scattering material for skins of pedestrian

- Visual Quality
  + Match Cameras: UE4 Editor-Espectator-Client
  + Match Weather: UE4 Editor-Espectator-Client
  + Improve the values of default weather for each Town
  + Screen Space Ambient Occlusion
  + Bloom
  + HDRI 

- Fixes
  + Fixed Zfighting in all Towns
  + Fixed Orientation of cars for each Static Mesh and Skeleton Mesh
  + Fixed LOD for all cars and motorbikes
  + Fixed vegetation LODs


